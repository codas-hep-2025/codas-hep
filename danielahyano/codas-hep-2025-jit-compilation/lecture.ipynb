{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882b9184",
   "metadata": {},
   "source": [
    "# JIT-compilation with Numba and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d159d",
   "metadata": {},
   "source": [
    "Let's consider the following quadratic formula:\n",
    "```python\n",
    "def quadratic_formula(a, b, c):\n",
    "    return (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e580ecb",
   "metadata": {},
   "source": [
    "\n",
    "What does this computation do? It gets more clear if we write this computation out:\n",
    "```python\n",
    "def pedantic_quadratic_formula(a, b, c):\n",
    "    tmp1 = np.negative(b)            # -b\n",
    "    tmp2 = np.square(b)              # b**2\n",
    "    tmp3 = np.multiply(4, a)         # 4*a\n",
    "    tmp4 = np.multiply(tmp3, c)      # tmp3*c\n",
    "    del tmp3\n",
    "    tmp5 = np.subtract(tmp2, tmp4)   # tmp2 - tmp4\n",
    "    del tmp2, tmp4\n",
    "    tmp6 = np.sqrt(tmp5)             # sqrt(tmp5)\n",
    "    del tmp5\n",
    "    tmp7 = np.add(tmp1, tmp6)        # tmp1 + tmp6\n",
    "    del tmp1, tmp6\n",
    "    tmp8 = np.multiply(2, a)         # 2*a\n",
    "    return np.divide(tmp7, tmp8)     # tmp7 / tmp8\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48703e94",
   "metadata": {},
   "source": [
    "\n",
    "There are **9(!)** elementwise operations that each runs a compiled loop, i.e.:\n",
    "```python\n",
    "tmp1 = np.negative(b)  \n",
    "tmp2 = np.square(b)\n",
    "...\n",
    "\n",
    "# is equivalent to\n",
    "n = len(b)\n",
    "tmp1 = np.empty(n)\n",
    "for i in range(n):  # (compiled loop)\n",
    "    tmp1[i] = -b[i]\n",
    "\n",
    "tmp2 = np.empty(n)\n",
    "for i in range(n):  # (compiled loop)\n",
    "    tmp2[i] = b[i] ** 2\n",
    "\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c1c6e",
   "metadata": {},
   "source": [
    "\n",
    "It would be much more efficient to apply all elementwise operations in a single loop:\n",
    "```python\n",
    "n = len(b)\n",
    "out = np.empty(n)\n",
    "for i in range(n):  # (compiled loop)\n",
    "    out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fdc85",
   "metadata": {},
   "source": [
    "Essentially, we're able to get rid of _intermediate_ arrays by \"fusing operations\" using just-in-time (JIT) compilation by applying these operations in a _single_ iteration over our data.\n",
    "\n",
    "Fusing operations is a tricky task however. There are a few ways to achieve this for array processing in Python, and I'd like to highlight two of them:\n",
    "\n",
    "- Numba: https://numba.pydata.org\n",
    "- JAX: https://github.com/jax-ml/jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf3f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax\n",
      "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax)\n",
      "  Downloading jaxlib-0.6.2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax)\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.26 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jax) (2.2.6)\n",
      "Collecting opt_einsum (from jax)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scipy>=1.12 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jax) (1.15.3)\n",
      "Downloading jax-0.6.2-py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.6.2-cp310-cp310-manylinux2014_x86_64.whl (89.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: opt_einsum, ml_dtypes, jaxlib, jax\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [jax]\u001b[32m3/4\u001b[0m [jax]ib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jax-0.6.2 jaxlib-0.6.2 ml_dtypes-0.5.1 opt_einsum-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9914c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Numba\n",
    "import numba as nb\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a37b3",
   "metadata": {},
   "source": [
    "Let's consider the quadratic formula example again, and compare the runtimes for NumPy, Numba, and JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6227aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "a = np.random.uniform(5, 10, 5_000_000)\n",
    "b = np.random.uniform(10, 20, 5_000_000)\n",
    "c = np.random.uniform(-0.1, 0.1, 5_000_000)\n",
    "\n",
    "\n",
    "# Setup quadratic formula\n",
    "def quadratic_formula(a, b, c):\n",
    "    return (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e56fd",
   "metadata": {},
   "source": [
    "NumPy case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab610b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.7 ms ± 1.53 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r3\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d0563",
   "metadata": {},
   "source": [
    "Numba case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdebf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit  # JIT compile!\n",
    "def quadratic_formula_numba(a, b, c):\n",
    "    n = a.shape[0]\n",
    "    out = np.empty(n)\n",
    "    for i in range(n):\n",
    "        out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9a0b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 ms ± 1.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7f5b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.6 ms ± 712 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b672a8",
   "metadata": {},
   "source": [
    "JAX case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1dc6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "a_jax = jnp.asarray(a)\n",
    "b_jax = jnp.asarray(b)\n",
    "c_jax = jnp.asarray(c)\n",
    "\n",
    "\n",
    "@jax.jit  # JIT compile!\n",
    "def quadratic_formula_jax(a, b, c):\n",
    "    return (-b + jnp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b550bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.6 ms ± 7.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_jax(a_jax, b_jax, c_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "410130fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.5 ms ± 336 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_jax(a_jax, b_jax, c_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047444",
   "metadata": {},
   "source": [
    "The first invocation for JAX & Numba took longer than consecutive ones. That's the compile time! Afterwards the compiled function is cached...\n",
    "\n",
    "But JAX is still much faster, why?\n",
    "\n",
    "One important difference is that JAX uses as many threads as it has access to. Numba is single-threaded, but can be multithreaded using `parallel=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de06e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)  # JIT compile with `parallel=True`!\n",
    "def quadratic_formula_numba_parallel(a, b, c):\n",
    "    n = a.shape[0]\n",
    "    out = np.empty(n)\n",
    "    for i in nb.prange(n):  # note: `range` -> `nb.prange`\n",
    "        out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7f4681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 ms ± 904 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_numba_parallel(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3529e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.5 ms ± 421 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_numba_parallel(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a60ef",
   "metadata": {},
   "source": [
    "Now we're roughly on-par with JAX and Numba with ~2-3ms runtime compared to NumPy's ~23ms.\n",
    "\n",
    "\n",
    "You might have noticed a fundamental difference between JAX and Numba in how those kernels are written: \n",
    "\n",
    "- Numba forces[<sup id=\"fn1-back\">1</sup>](#fn1) you to write _imperative_ code\n",
    "- JAX forces[<sup id=\"fn2-back\">2</sup>](#fn2) you to write _array-oriented_ code\n",
    "\n",
    "\n",
    "![image](https://raw.githubusercontent.com/jpivarski-talks/2023-12-18-hsf-india-tutorial-bhubaneswar/refs/heads/main/img/slow-fast-imperative-vectorized.svg)\n",
    "\n",
    "\n",
    "\n",
    "[<sup id=\"fn1\">1</sup>](#fn1-back) <sup>Can be written array-oriented with `nb.vectorize`.</sup> \n",
    "\n",
    "[<sup id=\"fn2\">2</sup>](#fn2-back) <sup>Can be written imperative with JAX's own loop primitives, e.g. `jax.lax.scan`.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c850a5",
   "metadata": {},
   "source": [
    "### How does JIT compilation even work? (JAX)\n",
    "\n",
    "Let's have a look at the JAX example, what does `jax.jit` do?\n",
    "\n",
    "It works in 4 steps:\n",
    "1. Stage out a `jax.jit`-decorated function into a new program using a JAX internal IR (JaxPr)\n",
    "2. Lower this IR (JaxPr) into the StableHLO IR\n",
    "3. Compile the StableHLO program with the XLA compiler\n",
    "4. Execute the compiled program\n",
    "\n",
    "Let's see those 4 steps in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b7e2075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f64[5000000]\u001b[39m b\u001b[35m:f64[5000000]\u001b[39m c\u001b[35m:f64[5000000]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22md\u001b[35m:f64[5000000]\u001b[39m = neg b\n",
      "    e\u001b[35m:f64[5000000]\u001b[39m = integer_pow[y=2] b\n",
      "    f\u001b[35m:f64[5000000]\u001b[39m = mul 4.0:f64[] a\n",
      "    g\u001b[35m:f64[5000000]\u001b[39m = mul f c\n",
      "    h\u001b[35m:f64[5000000]\u001b[39m = sub e g\n",
      "    i\u001b[35m:f64[5000000]\u001b[39m = sqrt h\n",
      "    j\u001b[35m:f64[5000000]\u001b[39m = add d i\n",
      "    k\u001b[35m:f64[5000000]\u001b[39m = mul 2.0:f64[] a\n",
      "    l\u001b[35m:f64[5000000]\u001b[39m = div j k\n",
      "  \u001b[34;1min \u001b[39;22m(l,) }\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the JaxPr (through tracing)\n",
    "traced = quadratic_formula_jax.trace(a_jax, b_jax, c_jax)\n",
    "print(traced.jaxpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d619151",
   "metadata": {},
   "source": [
    "This JaxPr looks a lot like the previously shown pedantic version of the quadratic formula (lecture part-2):\n",
    "\n",
    "```python\n",
    "def pedantic_quadratic_formula(a, b, c):\n",
    "    tmp1 = np.negative(b)            # -b\n",
    "    tmp2 = np.square(b)              # b**2\n",
    "    tmp3 = np.multiply(4, a)         # 4*a\n",
    "    tmp4 = np.multiply(tmp3, c)      # tmp3*c\n",
    "    del tmp3\n",
    "    tmp5 = np.subtract(tmp2, tmp4)   # tmp2 - tmp4\n",
    "    del tmp2, tmp4\n",
    "    tmp6 = np.sqrt(tmp5)             # sqrt(tmp5)\n",
    "    del tmp5\n",
    "    tmp7 = np.add(tmp1, tmp6)        # tmp1 + tmp6\n",
    "    del tmp1, tmp6\n",
    "    tmp8 = np.multiply(2, a)         # 2*a\n",
    "    return np.divide(tmp7, tmp8)     # tmp7 / tmp8\n",
    "```\n",
    "\n",
    "But instead of executing line-by-line we'll lower our JaxPr to StableHLO, and then compile it with XLA to fuse those kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4cfe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module @jit_quadratic_formula_jax attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
      "  func.func public @main(%arg0: tensor<5000000xf64>, %arg1: tensor<5000000xf64>, %arg2: tensor<5000000xf64>) -> (tensor<5000000xf64> {jax.result_info = \"result\"}) {\n",
      "    %0 = stablehlo.negate %arg1 : tensor<5000000xf64>\n",
      "    %1 = stablehlo.multiply %arg1, %arg1 : tensor<5000000xf64>\n",
      "    %cst = stablehlo.constant dense<4.000000e+00> : tensor<f64>\n",
      "    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f64>) -> tensor<5000000xf64>\n",
      "    %3 = stablehlo.multiply %2, %arg0 : tensor<5000000xf64>\n",
      "    %4 = stablehlo.multiply %3, %arg2 : tensor<5000000xf64>\n",
      "    %5 = stablehlo.subtract %1, %4 : tensor<5000000xf64>\n",
      "    %6 = stablehlo.sqrt %5 : tensor<5000000xf64>\n",
      "    %7 = stablehlo.add %0, %6 : tensor<5000000xf64>\n",
      "    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f64>\n",
      "    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f64>) -> tensor<5000000xf64>\n",
      "    %9 = stablehlo.multiply %8, %arg0 : tensor<5000000xf64>\n",
      "    %10 = stablehlo.divide %7, %9 : tensor<5000000xf64>\n",
      "    return %10 : tensor<5000000xf64>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Lower the JaxPr to StableHLO (still looks similar to our pedantic code)\n",
    "lowered = quadratic_formula_jax.lower(a_jax, b_jax, c_jax)\n",
    "print(lowered.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9ef8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compile the StableHLO program with XLA\n",
    "compiled = lowered.compile()\n",
    "# print(compiled.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3441d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00666984  0.00110459  0.00262948 ... -0.00037638  0.00067358\n",
      "  0.0079175 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Execute the compiled program\n",
    "print(compiled(a_jax, b_jax, c_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9ef0f",
   "metadata": {},
   "source": [
    "### Limitations of Numba\n",
    "\n",
    "You can not JIT-compile arbitrary Python functions. Numba can only JIT-compile a subset of Python, i.e. everything that's \"known\" to Numba as a type (mostly NumPy & NumPy operations).\n",
    "\n",
    "For more information, see: https://numba.readthedocs.io/en/stable/user/5minguide.html#will-numba-work-for-my-code.\n",
    "\n",
    "\n",
    "Check the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b32f5665",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of argument at /tmp/ipykernel_2330/463706393.py (1)\n\nFile \"../../../../../tmp/ipykernel_2330/463706393.py\", line 1:\n<source missing, REPL/exec in use?>\n\nDuring: Pass nopython_type_inference \n\nThis error may have been caused by the following argument(s):\n- argument 0: Cannot determine Numba type of <class 'dict'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m----> 8\u001b[0m \u001b[43msum_dict_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fails, because `dict` is not a known type for Numba\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numba/core/dispatcher.py:424\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    420\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 424\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numba/core/dispatcher.py:365\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of argument at /tmp/ipykernel_2330/463706393.py (1)\n\nFile \"../../../../../tmp/ipykernel_2330/463706393.py\", line 1:\n<source missing, REPL/exec in use?>\n\nDuring: Pass nopython_type_inference \n\nThis error may have been caused by the following argument(s):\n- argument 0: Cannot determine Numba type of <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "@nb.njit\n",
    "def sum_dict_values(d):\n",
    "    out = 0.\n",
    "    for v in d.values():\n",
    "        out += v\n",
    "    return out\n",
    "\n",
    "sum_dict_values({\"a\": 1.0, \"b\": 2.0, \"c\": 3.0})  # Fails, because `dict` is not a known type for Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ff828",
   "metadata": {},
   "source": [
    "### Limitations of JAX\n",
    "\n",
    "JAX infers the operations that are going to be run through a \"tracing step\". Essentially, JAX will run your program once with shallow array objects (no data, just metadata). That let's you JIT-compile all of Python, **but** you can't JIT-compile data-dependent operations.\n",
    "\n",
    "For more \"sharp bits\", see: https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html.\n",
    "\n",
    "Check the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d9a169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<float64[5]>with<DynamicJaxprTrace>\n"
     ]
    },
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function accumulate_if at /tmp/ipykernel_2330/3495820511.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument arr.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mprod(arr)\n\u001b[1;32m     12\u001b[0m array \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m4.\u001b[39m, \u001b[38;5;241m5.\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maccumulate_if\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Fails, because jnp.any(arr > 3) is not traceable!\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36maccumulate_if\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate_if\u001b[39m(arr):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(arr)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39many(arr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(arr)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/jax/_src/core.py:1661\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1661\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function accumulate_if at /tmp/ipykernel_2330/3495820511.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument arr.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "# Data-dependent operations are not traceable\n",
    "\n",
    "@jax.jit\n",
    "def accumulate_if(arr):\n",
    "    print(arr)\n",
    "    if jnp.any(arr > 3):\n",
    "        return jnp.sum(arr)\n",
    "    else:\n",
    "        return jnp.prod(arr)\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "print(accumulate_if(array))  # Fails, because jnp.any(arr > 3) is not traceable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be44fdc",
   "metadata": {},
   "source": [
    "Another limitation of JAX is that you can't JIT compile programs with unknown shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea4cac8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NonConcreteBooleanIndexError",
     "evalue": "Array boolean indices must be concrete; got bool[5]\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.NonConcreteBooleanIndexError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonConcreteBooleanIndexError\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(arr[arr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3.0\u001b[39m])\n\u001b[1;32m      6\u001b[0m array \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m4.\u001b[39m, \u001b[38;5;241m5.\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msum_greater_than_three\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Fails, because the output shape of `arr[arr > 3.0]` is not inferrable through tracing (without data)\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m, in \u001b[0;36msum_greater_than_three\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum_greater_than_three\u001b[39m(arr):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1083\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1083\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:657\u001b[0m, in \u001b[0;36m_getitem\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m--> 657\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:645\u001b[0m, in \u001b[0;36mrewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value, out_sharding)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(aval, core\u001b[38;5;241m.\u001b[39mDShapedArray) \u001b[38;5;129;01mand\u001b[39;00m aval\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m () \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    640\u001b[0m         dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, dtypes\u001b[38;5;241m.\u001b[39mbool_) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m    643\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 645\u001b[0m treedef, static_idx, dynamic_idx \u001b[38;5;241m=\u001b[39m \u001b[43msplit_index_for_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m internal_gather \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    647\u001b[0m     _gather, treedef\u001b[38;5;241m=\u001b[39mtreedef, static_idx\u001b[38;5;241m=\u001b[39mstatic_idx,\n\u001b[1;32m    648\u001b[0m     indices_are_sorted\u001b[38;5;241m=\u001b[39mindices_are_sorted, unique_indices\u001b[38;5;241m=\u001b[39munique_indices,\n\u001b[1;32m    649\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:738\u001b[0m, in \u001b[0;36msplit_index_for_jit\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX does not support string indexing; got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# Expand any (concrete) boolean indices. We can then use advanced integer\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# indexing logic to handle them.\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43m_expand_bool_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(idx)\n\u001b[1;32m    741\u001b[0m dynamic \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(leaves)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:1059\u001b[0m, in \u001b[0;36m_expand_bool_indices\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   1055\u001b[0m   abstract_i \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mget_aval(i)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mis_concrete(i):\n\u001b[1;32m   1058\u001b[0m   \u001b[38;5;66;03m# TODO(mattjj): improve this error by tracking _why_ the indices are not concrete\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNonConcreteBooleanIndexError(abstract_i)\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(i) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1061\u001b[0m   out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mbool\u001b[39m(i))\n",
      "\u001b[0;31mNonConcreteBooleanIndexError\u001b[0m: Array boolean indices must be concrete; got bool[5]\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.NonConcreteBooleanIndexError"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def sum_greater_than_three(arr):\n",
    "    return jnp.sum(arr[arr > 3.0])\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "print(sum_greater_than_three(array))  # Fails, because the output shape of `arr[arr > 3.0]` is not inferrable through tracing (without data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec1619",
   "metadata": {},
   "source": [
    "### Impure functions are dangerous with JIT compilation! (Numba & JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c2d80a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulate with `np.prod`: 120.0\n",
      "Accumulate with `np.sum`: 120.0 ...Hey, this should've been 15.0 instead!\n"
     ]
    }
   ],
   "source": [
    "do_sum = False\n",
    "\n",
    "@nb.njit\n",
    "def accumulate(arr):\n",
    "    if do_sum:\n",
    "        return np.sum(arr)\n",
    "    else:\n",
    "        return np.prod(arr)\n",
    "\n",
    "\n",
    "array = np.array([1., 2., 3., 4., 5.])\n",
    "print(\"Accumulate with `np.prod`:\", accumulate(array))\n",
    "\n",
    "# now we switch `do_sum` on!\n",
    "do_sum = True\n",
    "print(\"Accumulate with `np.sum`:\", accumulate(array), f\"...Hey, this should've been {np.sum(array)} instead!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e9718f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulate with `jnp.prod`: 120.0\n",
      "Accumulate with `jnp.sum`: 120.0 ...Hey, this should've been 15.0 instead!\n"
     ]
    }
   ],
   "source": [
    "do_sum = False\n",
    "\n",
    "@jax.jit\n",
    "def accumulate(arr):\n",
    "    if do_sum:\n",
    "        return jnp.sum(arr)\n",
    "    else:\n",
    "        return jnp.prod(arr)\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "\n",
    "print(\"Accumulate with `jnp.prod`:\", accumulate(array))\n",
    "\n",
    "# now we switch `do_sum` on!\n",
    "do_sum = True\n",
    "print(\"Accumulate with `jnp.sum`:\", accumulate(array), f\"...Hey, this should've been {jnp.sum(array)} instead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86c88",
   "metadata": {},
   "source": [
    "We can see why in the JAX case: that's because the traced program _never knew_ that there's a `sum` operation in the first place _and_ the compiled function is cached based on their input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4d6bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced program:\n",
      " { \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f64[5]\u001b[39m. \u001b[34;1mlet\u001b[39;22m b\u001b[35m:f64[]\u001b[39m = reduce_prod[axes=(0,)] a \u001b[34;1min \u001b[39;22m(b,) }\n",
      "\n",
      "HLO program:\n",
      " module @jit_accumulate attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
      "  func.func public @main(%arg0: tensor<5xf64>) -> (tensor<f64> {jax.result_info = \"result\"}) {\n",
      "    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f64>\n",
      "    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.multiply across dimensions = [0] : (tensor<5xf64>, tensor<f64>) -> tensor<f64>\n",
      "    return %0 : tensor<f64>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Traced program:\\n\", accumulate.trace(array).jaxpr)\n",
    "print()\n",
    "print(\"HLO program:\\n\", accumulate.lower(array).as_text()) # this is the program that get's compiled by XLA compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e023bd",
   "metadata": {},
   "source": [
    "On to the [project1.ipynb](project1.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cd834",
   "metadata": {},
   "source": [
    "### Auto-differentiation with JAX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afa1a0",
   "metadata": {},
   "source": [
    "Knowing the computational graph of a program (i.e. JaxPr) gives us the possibility to transform the program. JAX implements different _interpreters_ to execute the JaxPr of which one is able to replace every operation by its gradient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return 2.0 + jnp.sin(x)\n",
    "\n",
    "print(\"JaxPr:\")\n",
    "print(jax.make_jaxpr(fun)(1.0))\n",
    "print()\n",
    "\n",
    "grad_fun = jax.grad(fun)\n",
    "print(\"JaxPr (grad):\")\n",
    "print(jax.make_jaxpr(grad_fun)(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec961a58",
   "metadata": {},
   "source": [
    "Gradients are powerful! Many scientific problems involve gradient-based minimizations.\n",
    "\n",
    "Let's implement a gradient-based optimization on our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb959c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.key(42)\n",
    "\n",
    "true_a, true_b = 0.2, 1.1\n",
    "\n",
    "# function that we want to fit\n",
    "@jax.jit\n",
    "def function(x, a, b):\n",
    "    return b*x**2 - 4*a*x - b\n",
    "\n",
    "# generate true data with some noise\n",
    "def generate_data(rng):\n",
    "    x_key, noise_key = jax.random.split(rng)\n",
    "\n",
    "    xs = jax.random.uniform(x_key, (128, 1), minval=-3, maxval=3)\n",
    "    noise = jax.random.normal(noise_key, (128, 1)) * 0.15\n",
    "\n",
    "    ys = function(x=xs + noise, a=true_a, b=true_b)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# plot data\n",
    "xs, ys = generate_data(rng=rng)\n",
    "plt.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545921",
   "metadata": {},
   "source": [
    "We want to know what the true underlying `a` and `b` values are in this distribution. The next cell implements a gradient-based optimization to fit `function` to the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b637ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "# Just a struct that holds the parameters of the function\n",
    "class Params(NamedTuple):\n",
    "    a: jax.Array\n",
    "    b: jax.Array\n",
    "\n",
    "\n",
    "# Initialize parameters for the function (`a` and `b`)\n",
    "def init(rng) -> Params:\n",
    "    a_key, b_key = jax.random.split(rng)\n",
    "    a = jax.random.normal(a_key, ())\n",
    "    b = jax.random.normal(b_key, ())\n",
    "    return Params(a, b)\n",
    "\n",
    "\n",
    "# Compute the loss function (least squares error)\n",
    "def loss(params: Params, x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    pred = function(x=x, a=params.a, b=params.b)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "\n",
    "# Perform one gradient descent update step on params using the given data. (~SGD)\n",
    "@jax.jit\n",
    "def update(params: Params, x: jax.Array, y: jax.Array) -> Params:\n",
    "    # Computes the gradients of the loss function with respect to the parameters\n",
    "    grads = jax.grad(loss)(params, x, y)\n",
    "\n",
    "    # Define a step function that updates the parameters\n",
    "    def step(param, grad):\n",
    "      return param - 0.005 * grad  # 0.005 := learning rate\n",
    "\n",
    "    # Apply the step function to each parameter\n",
    "    return jax.tree.map(step, params, grads)\n",
    "\n",
    "\n",
    "# Run the optimization\n",
    "params = init(rng)\n",
    "for _ in range(500):\n",
    "    params = update(params, xs, ys)\n",
    "\n",
    "\n",
    "print(f\"True parameters  : a={true_a:.2f}, b={true_b:.2f}\")\n",
    "print(f\"Fitted parameters: a={params.a:.2f}, b={params.b:.2f}\")\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "pred_ys = function(x=xs, a=params.a, b=params.b)\n",
    "plt.plot(xs, pred_ys, \".\", c='red', label=f'Fit result: a={params.a:.2f}, b={params.b:.2f}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8a09f",
   "metadata": {},
   "source": [
    "This is the key ingredient for training neural networks, see more at tomorrow's ML lecture by Liv!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
